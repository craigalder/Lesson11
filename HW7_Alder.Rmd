---
title: "HW7"
author: "Craig Alder"
date: "4/5/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Recall the NHANES dataset that we used in Lesson 10. 
```{r}
library(tidyverse)
library(class)
library(rpart)
library(NHANES)
library(RColorBrewer)
library(plot3D)
library(parallel)
library(randomForestSRC)
library(ggRandomForests)
library(mosaic)
library(ggplot2)
library(HistData)
library(car)
library(stargazer)

# Create the NHANES dataset again

people <- NHANES %>% dplyr::select(DaysMentHlthBad, PhysActive, AlcoholYear, Age, SleepHrsNight, SleepTrouble) 
#%>% na.omit()
glimpse(people)
class(people)

# Convert back to dataframe
people <- as.data.frame(people)
glimpse(people)

# Convert factors to numeric - the packages just seem to work better that way
#health
people$DaysMentHlthBad <- as.numeric(people$DaysMentHlthBad)
people$PhysActive <- as.numeric(people$PhysActive)
people$AlcoholYear <- as.numeric(people$AlcoholYear)
#demographics
people$Age <- as.numeric(people$Age)

#sleep
people$SleepTrouble <- as.numeric(people$SleepTrouble)
people$SleepHrsNight <- as.numeric(people$SleepHrsNight)

people <- na.omit(people)

glimpse(people)
```


1. In the dataset there is a discrete variable called SleepTrouble indicating whether each participant has trouble sleeping or not. You are going to build a set of classifiers for this dependent variable. You may use any (set of) independent variable(s) you like except for the variable callsed SleepHrsNight. 

For each of the model types (null model, logistic regression, decision tree, random forest, k-nearest neighbor) do the following:

Null Model
**Building model**
```{r}
RegModel1 <- 
  lm(SleepTrouble~DaysMentHlthBad + PhysActive + AlcoholYear + Age,
   data=people)
summary(RegModel1)
```
**Effectiveness**
My current model explains about 6% of the variance in the outcome variable. Each independent variable is statistically significant at p<.01 except the measure for physical activity. This model doesn't explain very much right now, so it will be interesting to see how different methods change the predictive ability of these variables.

**Visualization**
```{r}
# diagnostics using residual plots
residualPlots(RegModel1)
#added variable plots
avPlots(RegModel1, id.n=3, id.cex=0.7)
# run the qq-plot, observations with large residuals
qqPlot(RegModel1, id.n=3)
#identify highly influential points
influenceIndexPlot(RegModel1, id.n=3)
#influence plot
influencePlot(RegModel1, id.n=3)
#heteroscedasticity
ncvTest(RegModel1)
#multicollinearity
vif(RegModel1)
``` 
**Interpret the results. What have you learned about people's sleeping habits?**
While I am not quite sure how to interpret these graphs with a binary outcome in a least squares regression, it does seem like there are some problems with assumptions about heteroskedasticity and normality. Logistic regression will probably make a lot more sense (at least as far as visualizing).

The regression table does tell us that, controlling for the other variables, a 1-unit increase in bad mental health days is associated with a .01 increase in the likelihood of having trouble with sleep. A 1-unit increase in yearly alcohol consumption is associated with a .0001 decrease in the likelihood of struggling with sleep, controlling for all other variables. A 1-unit increase in age is associated with a .003 increase in likelihood of sleep struggle, controlling for the other variables. All these relationships are significant at p<.001. Multicollinearity does not seem to be a problem, and there are no extreme values for the outcome. 

1D. Interpret the results. What have you learned about people's sleeping habits?

Logistic Regression
**Building model**
```{r}
people$SleepTrouble[people$SleepTrouble == 1 ] <- 0
people$SleepTrouble[people$SleepTrouble == 2 ] <- 1
train <- people[1:8000,]
test <- people[8001:10000,]
model <- glm(SleepTrouble~DaysMentHlthBad + PhysActive + AlcoholYear + Age,family=binomial(link='logit'),data=train)
##Model goes where ~ is, just like lsq
summary(model)
##https://www.r-bloggers.com/how-to-perform-a-logistic-regression-in-r/
```
**Effectiveness**
A 1 unit increase in bad mental health days is associated with a .057 increase in the logit of sleep trouble, significant at p<.001. Age and alcohol also have significant relationships, while the coefficient for physical activity is quite high.
**Visualization**
```{r, include=TRUE}

```

**Interpret the results. What have you learned about people's sleeping habits?**

Decision Tree
**Building model**
```{r}
library(rpart)
library(tidyverse)

_rpart_(*formula*, _data =_, _method =_, _control =_) where 
* *formula* is in the format Y ~ X1 + X2 + ... + Xp
* _data =_ specifies the data frame
* _method = "anova"_ for a regression tree
* _method = "class"_ for a decision tree
* _control =_ a series of optional parameters that controls the process of tree growth.

The output is an object called *fit*. 
```
**Effectiveness**
**Visualization**
```{r}
printcp(fit)   | display the cp table                         |
| plotcp(fit)    | plot cross-validation results                |
| rsp.rpart(fit) | plot approx. R-squared for 2 different splits|
| print(fit)     | print results                                |
| summary(fit)   | detailed results including surrogate splits  |
| plot(fit)      | plot decision tree                           |
| text(fit)  

##Pruning the tree -- follow all the steps in lesson 9
```
**Interpret the results. What have you learned about people's sleeping habits?**

Random forest
**Building model**
```{r}
library(randomForest)

set.seed(131)
# Random Forest for the ozone dataset
fitallrf <- randomForest(O3 ~ ., data = ozone, importance = TRUE)
impallrf <- importance(fitallrf)
```
**Effectiveness**
**Visualization**
```{r}
# view the results
print(fitallrf)
importance(fitallrf)
```
**Interpret the results. What have you learned about people's sleeping habits?**

k-nearest neighbor
**Building model**
```{r}
# Apply knn procedure to predict Diabetes

# Let's try different values of k to see how that affects performance

#knn.1 <- knn(train = people, test = people, cl = as.numeric(people$Diabetes), k = 1)
#knn.3 <- knn(train = people, test = people, cl = people$Diabetes, k = 3)
#knn.5 <- knn(train = people, test = people, cl = people$Diabetes, k = 5)
#knn.20 <- knn(train = people, test = people, cl = people$Diabetes, k = 20)

#knn.1
```
**Effectiveness**
Now let's see how well it classifies
```{r}

# Calculate the percent predicted correctly

#100*sum(people$Diabetes == knn.1)/length(knn.1)
#100*sum(people$Diabetes == knn.3)/length(knn.3)
#100*sum(people$Diabetes == knn.5)/length(knn.5)
#100*sum(people$Diabetes == knn.20)/length(knn.20)

```

```{r}

# Another way to look at success rate against increasing k

#table(knn.1, people$Diabetes)
#table(knn.3, people$Diabetes)
#table(knn.5, people$Diabetes)
#table(knn.20, people$Diabetes)
```

2. Repeat problem 1 except now you are to use the quantitative variable called SleepHrsNight. The model types are as follows: null model, multiple regression, regression tree, random forest.

I unfortunately just ran out of time this week and won't be able to complete models with SleepHrsNight.

The link to my Github account is [https://github.com/craigalder](https://github.com/craigalder). The link to my repository for this assignment is [https://github.com/craigalder/Lesson11.git](https://github.com/craigalder/Lesson11.git).
